---
title: 《机器学习与R语言》学习笔记02：朴素贝叶斯
date: '2019-04-25'
categories:
  - R
tags:
  - 笔记
  - 机器学习
slug: mlr-note-nb
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(message = FALSE)
```

书中的第二个例子是利用朴素贝叶斯算法判断垃圾短信。

首先载入需要用到的包：
```{r warning = FALSE}
library(tidyverse) # 清洗数据
library(here) # 设置数据文件路径
library(tidytext) # 分词及创建稀疏矩阵
library(e1071) # 建模
library(gmodels) # 评估模型
```

在清洗数据的时候遇到一定的困难，因为书中是用`tm`包进行文本处理的，而我完全没有用过这个包（甚至也没有装这个包），所以看书中的代码就只能凭感觉脑补了。不过，还好，最后还是成功写出了`tidyverse`化的数据清洗代码，如下：

```{r eval = FALSE}
sms <- read_csv(here('content', 'post', 'data', '02-sms_spam.csv')) %>% 
  mutate(type = factor(type),
         row = row_number()) %>% 
  unnest_tokens(word, text) %>% 
  anti_join(stop_words) %>% 
  filter(!str_detect(word, '\\d')) %>% 
  cast_sparse(row, word) %>% 
  as.matrix() %>% 
  as_tibble() %>% 
  select(which(colSums(.) > 4)) %>% 
  bind_cols(read_csv(here('data', '02-sms_spam.csv')) %>% 
              mutate(type = factor(type),
                     row = row_number()) %>% 
              unnest_tokens(word, text) %>% 
              anti_join(stop_words) %>% 
              filter(!str_detect(word, '\\d')) %>%
              select(-3) %>% 
              distinct()) %>% 
  mutate_if(is.numeric, factor, levels = c(0, 1), labels = c('No', 'Yes'))
```

虽然是很长一串，但还是要比书中的代码少10来行的，而且连贯性和可读性也更高，最重要的是，只需要命名一个变量。

分解一下：

原始数据是这样的：

```{r}
(sms <- read_csv(here('content', 'post', 'data', '02-sms_spam.csv')))
```

随后将标签变量`type`变为因子型，并新增`row`变量，记录行数：

```{r}
sms <- read_csv(here('content', 'post', 'data', '02-sms_spam.csv')) %>% 
  mutate(type = factor(type),
         row = row_number())
```

然后利用`tidytext`包中的`unnest_token`函数进行分词，利用`anti_join`函数去掉停用词，再利用`filter`和`str_detect`的组合去掉数字。此时的数据是这样的：

```{r}
(sms <- read_csv(here('content', 'post', 'data', '02-sms_spam.csv')) %>% 
  mutate(type = factor(type),
         row = row_number()) %>% 
  unnest_tokens(word, text) %>% 
  anti_join(stop_words) %>% 
  filter(!str_detect(word, '\\d')))
```

其中共涉及到`r unique(sms$word) %>% length()`个词汇。

这时遇到了困难，因为需要把数据整成稀疏矩阵，也就是要做到每个词自成一列，假如某条短信内出现了该词，则记为1，没有出现的话，则记为0。数据一共5000多行，而词汇共有7000多个，即要整理出一个5000*7000的矩阵或数据框。一开始想尝试用`tidyr`包来解决这个问题，结果发现生成了一个5GB的数据框，虽然也能把问题解决，但这个方法太慢了。看书里的方法，`tm`包中是有相关的函数来进行这一步转换的；去网上查，发现`Matrix`包也能解决这个问题，但它们都会破坏代码的完整性。后来想到，`tidytext`应该不会没有处理这种问题的函数，看了下，果然有个`cast_sparse`函数，可以调用`Matrix`包中的`sparseMatrix`函数。此时问题还没有完全解决，以为`cast_sparse`函数生成的矩阵是一个class为`dgCMatrix`的矩阵，没法直接转为数据框。又在网上查了下，发现可以先将其转为矩阵，然后再转为数据框。此时的部分数据是这样的：

```{r}
(sms <- read_csv(here('content', 'post', 'data', '02-sms_spam.csv')) %>% 
  mutate(type = factor(type),
         row = row_number()) %>% 
  unnest_tokens(word, text) %>% 
  anti_join(stop_words) %>% 
  filter(!str_detect(word, '\\d')) %>% 
  cast_sparse(row, word) %>% 
  as.matrix() %>% 
  as_tibble())
```

下一步是按着书里的标准，去掉出现频次较低的词汇，仅保留至少出现过5次的词汇。这里也遇到点小困难，本来是想用`select_*`系列的函数剔除低频词汇的，但没有成功，最后在网上查到了更为简单的方式。这时变量就从7440变成了1312，数据框的大小也从300多MB减少到了50多MB：

```{r}
(sms <- read_csv(here('content', 'post', 'data', '02-sms_spam.csv')) %>% 
  mutate(type = factor(type),
         row = row_number()) %>% 
  unnest_tokens(word, text) %>% 
  anti_join(stop_words) %>% 
  filter(!str_detect(word, '\\d')) %>% 
  cast_sparse(row, word) %>% 
  as.matrix() %>% 
  as_tibble() %>% 
  select(which(colSums(.) > 4)))
```

这时一个稀疏矩阵就建好了，但数据中还没有标签，所以我又用一大段重复的代码把行数和标签并了进去。暂时没想到更简单的方式可以在不打断代码的前提下完成同样的事情。最后一步是按照书中讲到的，把所有预测变量变为因子型：

```{r}
sms <- read_csv(here('content', 'post', 'data', '02-sms_spam.csv')) %>% 
  mutate(type = factor(type),
         row = row_number()) %>% 
  unnest_tokens(word, text) %>% 
  anti_join(stop_words) %>% 
  filter(!str_detect(word, '\\d')) %>% 
  cast_sparse(row, word) %>% 
  as.matrix() %>% 
  as_tibble() %>% 
  select(which(colSums(.) > 4)) %>% 
  bind_cols(read_csv(here('content', 'post', 'data', '02-sms_spam.csv')) %>% 
              mutate(type = factor(type),
                     row = row_number()) %>% 
              unnest_tokens(word, text) %>% 
              anti_join(stop_words) %>% 
              filter(!str_detect(word, '\\d')) %>%
              select(-3) %>% 
              distinct()) %>% 
  mutate_if(is.numeric, factor, levels = c(0, 1), labels = c('No', 'Yes'))
```

数据已经清洗好了，可以创建训练数据集和测试数据集了：

```{r}
set.seed(0424)
sms_train <- sms %>% sample_n(4169)
sms_test <- sms %>% setdiff(sms_train)
```

这里也遇到个问题。在去掉频次较少的词汇前，也就是有7000多列时，`sample_n`函数会报错，但去掉那些词汇后，就没有问题了。猜测使用`sample_n`函数时，数据的变量数不能大于参数n的值？

然后就可以建模了。模型里的训练数据去掉了最后两列（行数和标签），而且需要注意的是，因为词汇中包括type这个词，所以本来的`type`变量名被自动变更为`type1`了。

```{r}
sms_class <- naiveBayes(sms_train[, -1313:-1314], sms_train$type1)
sms_pred <- predict(sms_class, sms_test)
```

用测试数据评估一下模型：

```{r}
CrossTable(sms_pred, sms_test$type1, 
           prop.chisq = FALSE, prop.t = FALSE,
           dnn = c('predicted', 'actual'))
```

应该还挺不错的。

按书里的方式，更改`laplace`参数再试一下：

```{r}
sms_class1 <- naiveBayes(sms_train[, -1313:-1314], sms_train$type1, laplace = 1)
sms_pred1 <- predict(sms_class1, sms_test)

CrossTable(sms_pred1, sms_test$type1, 
           prop.chisq = FALSE, prop.t = FALSE,
           dnn = c('predicted', 'actual'))
```

此时模型确实得到了一定程度的优化，因为虽然多看了两条垃圾短信，但少错过了一条非垃圾短信。
