

  
    
  


  




  


  

<!DOCTYPE html>
<html lang="en-us">
  <head>
    
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="generator" content="Hugo 0.54.0 with theme Tranquilpeak 0.4.3-BETA">
    <title>使用R语言文本挖掘技术应对英语考试的粗浅想法及其他</title>
    <meta name="author" content="孟祥良">
    <meta name="keywords" content="">

    <link rel="icon" href="/favicon.png">
    

    
    <meta name="description" content="学习与考试学习是由经验引起的相对持久的变化，而考试，一方面是对习得的知识与技能的掌握情况的测试，另一方面也是对能否进入下一阶段学习或工作的考核。虽然学习能力受个人的智力水平以及学习方法的影响，但一般而言，学习是没有捷径的；但考试却不一样，在应付考试的时候，确实存在一些捷径可走。且以我个人的经历举三个例子。
高中的时候，我的立体几何学得不是很好，对于如何解立体几何大题完全摸不着门路。然而有一天，我在一本辅导书中看到一种用空间向量解立体几何大题的方法，而且相当简单易学，做了几道类似的题，我就掌握了这个方法。随后在整个高中阶段，我就靠着这一捷径，在并不具备足够的立体几何知识的前提下做到了立体几何大题从来不丢分。
我本科是自考的英语专业，其中有一门课，叫《英美文学选读》。这门课应该是所有20多门课中最有价值的一门，同时也是最难的一门。教材中介绍了英国与美国的70多位著名的作家以及他们的代表作。我第一次没能考过，因为内容实在是太多了，不好把握重点。后来，我找了几份历年的真题，看过之后才发现，原来70多位作者当中，只有30多位会在考试中考到，这一下就把任务量缩减了一半。虽然难度还是挺大的，但第二次终于顺便通过了。我又一次找到了考试的捷径，同时也第一次意识到真题对于考试的重要性。
拿到本科毕业证之后，我就开始着手考研。专业课所涉及的内容不可谓不多，如果想全面的复习，时间肯定不够，只能有所侧重的进行复习。有了先前的经验，我开始反复地研究真题。哪些知识点会以选择题的形式考察，哪些知识点会以大题的形式考察，哪些部分的内容考试从来不涉及，可以忽略，哪些部分的内容频繁考到，必须重点记忆？经过反复地分析，我根据真题总结出了一份比较精简的笔记，并在考试中拿到了一个还算满意的分数。如果仅仅按大纲来复习的话，肯定会浪费很多时间。
从我个人经历来看，考试确实是有捷径可循，而这个捷径往往跟历年的真题有关。但是对于大部分考试来说，可用于分析的样本量太低，而且找不到客观的指标，只能通过自己的主观判断，因此在知识点的选取上难免会有一些偏差。但有一门考试却不存在这两个问题，既有足够大的样本量可供分析，又能产生客观的指标。很明显，这门考试就是大家“喜闻乐见”英语考试。
上大学之前，语数外三科同等重要；而上大学之后，大多数专业应该都不用学语文了，文科的一般也不用学数学了，只有英语，坚挺地为几乎所有专业所必学，并在很多学校作为毕业的条件之一。从这一点也可以看出来英语的重要性。学好英语不是一件轻松的事情，需要长时间的积累。虽然我恐怕不会再参加任何类型的英语考试了，但是未了将来的更好发展，我基本上保持着每天10K的英文阅读量。不过，应付英语考试就是另外一回事了，完全有可能出现我在备考《英美文学选读》时的情况。此外，我还想强调一点，我们这一辈子为了应付考试浪费了太多的生命了，也许功利地应付考试，反而倒是一种非功利的学习行为。
要准备英语考试，大部分人的第一反应应该都是要背单词！各个级别的英语考试都有那么几千到上万的单词需要背诵，绝大多数考生都或多或少地受到过背单词的折磨。背了忘，忘了再背，背了再忘，反反复复，没有休止。一方面，对单词的记忆会随着时间的流逝而消退，另一方面，几千个单词之间往往也会产生各种干扰，导致遗忘。如果需要背诵的单词量大大减少的话，情况就会好得多了。那现在的问题就是，各级别英语考试大纲中的那些单词，真的都需要背下来吗？
社会学中有个著名的80/20定律，即这个世界80%的财富都集中在20%的人手里。对于英语试卷这一定律可能也适用，即80%的文本都是由那20%最常见的单词组成。如果真是这样的话，那只需要原先五分之一的时间（也许更少，因为这时的干扰也更少），就足以应付英语考试了。市面上其实早就有了通过词频来排列单词供人背诵的参考书，但是这些书的销量似乎并不是很好，这大概是因为这些书只是笼统的把全部文本都拿来进行统计，不够细致，也没有针对性。如果能针对不同类型的试题，甚至针对这些试题的不同部分，统计出来词频，肯定会有更好的指导作用。
针对高考英语阅读题的文本分析为了进行验证与分析，我从网上找到了最近9年的天津高考英语试题文档。我把其中的阅读部分复制了出来，按年份保存在了txt文档中，然后利用tidyverse和tidytext包，制作了两份csv格式句子库文档，其中一份是以全部阅读文本为基础生成的，另一份则只针对阅读题的题干而生成。现在先载入需要的包，并把这两份数据导入：
library(tidyverse)library(tidytext)library(pipeR)library(magrittr)library(knitr)reading &lt;- read.csv(&#39;reading.csv&#39;, stringsAsFactors = FALSE)question &lt;- read.csv(&#39;question.csv&#39;, stringsAsFactors = FALSE)因为readr包中的函数对中文的支持不是很好，所以我这里使用了R自带的函数。先看一下这两份数据：
reading %&gt;% head() %&gt;% kable()yearregionpassagesentencesentence_low2007天津AThe city of Rome has passed a new law to prevent cruelty to animals.the city of rome has passed a new law to prevent cruelty to animals.">
    <meta property="og:description" content="学习与考试学习是由经验引起的相对持久的变化，而考试，一方面是对习得的知识与技能的掌握情况的测试，另一方面也是对能否进入下一阶段学习或工作的考核。虽然学习能力受个人的智力水平以及学习方法的影响，但一般而言，学习是没有捷径的；但考试却不一样，在应付考试的时候，确实存在一些捷径可走。且以我个人的经历举三个例子。
高中的时候，我的立体几何学得不是很好，对于如何解立体几何大题完全摸不着门路。然而有一天，我在一本辅导书中看到一种用空间向量解立体几何大题的方法，而且相当简单易学，做了几道类似的题，我就掌握了这个方法。随后在整个高中阶段，我就靠着这一捷径，在并不具备足够的立体几何知识的前提下做到了立体几何大题从来不丢分。
我本科是自考的英语专业，其中有一门课，叫《英美文学选读》。这门课应该是所有20多门课中最有价值的一门，同时也是最难的一门。教材中介绍了英国与美国的70多位著名的作家以及他们的代表作。我第一次没能考过，因为内容实在是太多了，不好把握重点。后来，我找了几份历年的真题，看过之后才发现，原来70多位作者当中，只有30多位会在考试中考到，这一下就把任务量缩减了一半。虽然难度还是挺大的，但第二次终于顺便通过了。我又一次找到了考试的捷径，同时也第一次意识到真题对于考试的重要性。
拿到本科毕业证之后，我就开始着手考研。专业课所涉及的内容不可谓不多，如果想全面的复习，时间肯定不够，只能有所侧重的进行复习。有了先前的经验，我开始反复地研究真题。哪些知识点会以选择题的形式考察，哪些知识点会以大题的形式考察，哪些部分的内容考试从来不涉及，可以忽略，哪些部分的内容频繁考到，必须重点记忆？经过反复地分析，我根据真题总结出了一份比较精简的笔记，并在考试中拿到了一个还算满意的分数。如果仅仅按大纲来复习的话，肯定会浪费很多时间。
从我个人经历来看，考试确实是有捷径可循，而这个捷径往往跟历年的真题有关。但是对于大部分考试来说，可用于分析的样本量太低，而且找不到客观的指标，只能通过自己的主观判断，因此在知识点的选取上难免会有一些偏差。但有一门考试却不存在这两个问题，既有足够大的样本量可供分析，又能产生客观的指标。很明显，这门考试就是大家“喜闻乐见”英语考试。
上大学之前，语数外三科同等重要；而上大学之后，大多数专业应该都不用学语文了，文科的一般也不用学数学了，只有英语，坚挺地为几乎所有专业所必学，并在很多学校作为毕业的条件之一。从这一点也可以看出来英语的重要性。学好英语不是一件轻松的事情，需要长时间的积累。虽然我恐怕不会再参加任何类型的英语考试了，但是未了将来的更好发展，我基本上保持着每天10K的英文阅读量。不过，应付英语考试就是另外一回事了，完全有可能出现我在备考《英美文学选读》时的情况。此外，我还想强调一点，我们这一辈子为了应付考试浪费了太多的生命了，也许功利地应付考试，反而倒是一种非功利的学习行为。
要准备英语考试，大部分人的第一反应应该都是要背单词！各个级别的英语考试都有那么几千到上万的单词需要背诵，绝大多数考生都或多或少地受到过背单词的折磨。背了忘，忘了再背，背了再忘，反反复复，没有休止。一方面，对单词的记忆会随着时间的流逝而消退，另一方面，几千个单词之间往往也会产生各种干扰，导致遗忘。如果需要背诵的单词量大大减少的话，情况就会好得多了。那现在的问题就是，各级别英语考试大纲中的那些单词，真的都需要背下来吗？
社会学中有个著名的80/20定律，即这个世界80%的财富都集中在20%的人手里。对于英语试卷这一定律可能也适用，即80%的文本都是由那20%最常见的单词组成。如果真是这样的话，那只需要原先五分之一的时间（也许更少，因为这时的干扰也更少），就足以应付英语考试了。市面上其实早就有了通过词频来排列单词供人背诵的参考书，但是这些书的销量似乎并不是很好，这大概是因为这些书只是笼统的把全部文本都拿来进行统计，不够细致，也没有针对性。如果能针对不同类型的试题，甚至针对这些试题的不同部分，统计出来词频，肯定会有更好的指导作用。
针对高考英语阅读题的文本分析为了进行验证与分析，我从网上找到了最近9年的天津高考英语试题文档。我把其中的阅读部分复制了出来，按年份保存在了txt文档中，然后利用tidyverse和tidytext包，制作了两份csv格式句子库文档，其中一份是以全部阅读文本为基础生成的，另一份则只针对阅读题的题干而生成。现在先载入需要的包，并把这两份数据导入：
library(tidyverse)library(tidytext)library(pipeR)library(magrittr)library(knitr)reading &lt;- read.csv(&#39;reading.csv&#39;, stringsAsFactors = FALSE)question &lt;- read.csv(&#39;question.csv&#39;, stringsAsFactors = FALSE)因为readr包中的函数对中文的支持不是很好，所以我这里使用了R自带的函数。先看一下这两份数据：
reading %&gt;% head() %&gt;% kable()yearregionpassagesentencesentence_low2007天津AThe city of Rome has passed a new law to prevent cruelty to animals.the city of rome has passed a new law to prevent cruelty to animals.">
    <meta property="og:type" content="blog">
    <meta property="og:title" content="使用R语言文本挖掘技术应对英语考试的粗浅想法及其他">
    <meta property="og:url" content="/2018/03/tackle-english-exam-with-text-mining/">
    <meta property="og:site_name" content="RPG">
    <meta name="twitter:card" content="summary">
    <meta name="twitter:title" content="RPG">
    <meta name="twitter:description" content="学习与考试学习是由经验引起的相对持久的变化，而考试，一方面是对习得的知识与技能的掌握情况的测试，另一方面也是对能否进入下一阶段学习或工作的考核。虽然学习能力受个人的智力水平以及学习方法的影响，但一般而言，学习是没有捷径的；但考试却不一样，在应付考试的时候，确实存在一些捷径可走。且以我个人的经历举三个例子。
高中的时候，我的立体几何学得不是很好，对于如何解立体几何大题完全摸不着门路。然而有一天，我在一本辅导书中看到一种用空间向量解立体几何大题的方法，而且相当简单易学，做了几道类似的题，我就掌握了这个方法。随后在整个高中阶段，我就靠着这一捷径，在并不具备足够的立体几何知识的前提下做到了立体几何大题从来不丢分。
我本科是自考的英语专业，其中有一门课，叫《英美文学选读》。这门课应该是所有20多门课中最有价值的一门，同时也是最难的一门。教材中介绍了英国与美国的70多位著名的作家以及他们的代表作。我第一次没能考过，因为内容实在是太多了，不好把握重点。后来，我找了几份历年的真题，看过之后才发现，原来70多位作者当中，只有30多位会在考试中考到，这一下就把任务量缩减了一半。虽然难度还是挺大的，但第二次终于顺便通过了。我又一次找到了考试的捷径，同时也第一次意识到真题对于考试的重要性。
拿到本科毕业证之后，我就开始着手考研。专业课所涉及的内容不可谓不多，如果想全面的复习，时间肯定不够，只能有所侧重的进行复习。有了先前的经验，我开始反复地研究真题。哪些知识点会以选择题的形式考察，哪些知识点会以大题的形式考察，哪些部分的内容考试从来不涉及，可以忽略，哪些部分的内容频繁考到，必须重点记忆？经过反复地分析，我根据真题总结出了一份比较精简的笔记，并在考试中拿到了一个还算满意的分数。如果仅仅按大纲来复习的话，肯定会浪费很多时间。
从我个人经历来看，考试确实是有捷径可循，而这个捷径往往跟历年的真题有关。但是对于大部分考试来说，可用于分析的样本量太低，而且找不到客观的指标，只能通过自己的主观判断，因此在知识点的选取上难免会有一些偏差。但有一门考试却不存在这两个问题，既有足够大的样本量可供分析，又能产生客观的指标。很明显，这门考试就是大家“喜闻乐见”英语考试。
上大学之前，语数外三科同等重要；而上大学之后，大多数专业应该都不用学语文了，文科的一般也不用学数学了，只有英语，坚挺地为几乎所有专业所必学，并在很多学校作为毕业的条件之一。从这一点也可以看出来英语的重要性。学好英语不是一件轻松的事情，需要长时间的积累。虽然我恐怕不会再参加任何类型的英语考试了，但是未了将来的更好发展，我基本上保持着每天10K的英文阅读量。不过，应付英语考试就是另外一回事了，完全有可能出现我在备考《英美文学选读》时的情况。此外，我还想强调一点，我们这一辈子为了应付考试浪费了太多的生命了，也许功利地应付考试，反而倒是一种非功利的学习行为。
要准备英语考试，大部分人的第一反应应该都是要背单词！各个级别的英语考试都有那么几千到上万的单词需要背诵，绝大多数考生都或多或少地受到过背单词的折磨。背了忘，忘了再背，背了再忘，反反复复，没有休止。一方面，对单词的记忆会随着时间的流逝而消退，另一方面，几千个单词之间往往也会产生各种干扰，导致遗忘。如果需要背诵的单词量大大减少的话，情况就会好得多了。那现在的问题就是，各级别英语考试大纲中的那些单词，真的都需要背下来吗？
社会学中有个著名的80/20定律，即这个世界80%的财富都集中在20%的人手里。对于英语试卷这一定律可能也适用，即80%的文本都是由那20%最常见的单词组成。如果真是这样的话，那只需要原先五分之一的时间（也许更少，因为这时的干扰也更少），就足以应付英语考试了。市面上其实早就有了通过词频来排列单词供人背诵的参考书，但是这些书的销量似乎并不是很好，这大概是因为这些书只是笼统的把全部文本都拿来进行统计，不够细致，也没有针对性。如果能针对不同类型的试题，甚至针对这些试题的不同部分，统计出来词频，肯定会有更好的指导作用。
针对高考英语阅读题的文本分析为了进行验证与分析，我从网上找到了最近9年的天津高考英语试题文档。我把其中的阅读部分复制了出来，按年份保存在了txt文档中，然后利用tidyverse和tidytext包，制作了两份csv格式句子库文档，其中一份是以全部阅读文本为基础生成的，另一份则只针对阅读题的题干而生成。现在先载入需要的包，并把这两份数据导入：
library(tidyverse)library(tidytext)library(pipeR)library(magrittr)library(knitr)reading &lt;- read.csv(&#39;reading.csv&#39;, stringsAsFactors = FALSE)question &lt;- read.csv(&#39;question.csv&#39;, stringsAsFactors = FALSE)因为readr包中的函数对中文的支持不是很好，所以我这里使用了R自带的函数。先看一下这两份数据：
reading %&gt;% head() %&gt;% kable()yearregionpassagesentencesentence_low2007天津AThe city of Rome has passed a new law to prevent cruelty to animals.the city of rome has passed a new law to prevent cruelty to animals.">
    
    

    
    

    
      <meta property="og:image" content="//www.gravatar.com/avatar/9a7089fad6e7d2ebee69f9659db0c484?s=640">
    

    
    
    

    

    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha256-eZrrJcwDc/3uDhsdt61sL2oOBY362qM3lon1gyExkL0=" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.4/jquery.fancybox.min.css" integrity="sha256-vuXZ9LGmmwtjqFX1F+EKin1ThZMub58gKULUyf0qECk=" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.4/helpers/jquery.fancybox-thumbs.min.css" integrity="sha256-SEa4XYAHihTcEP1f5gARTB2K26Uk8PsndQYHQC1f4jU=" crossorigin="anonymous" />
    
    
    <link rel="stylesheet" href="/css/style-jsjn0006wyhpyzivf6yceb31gvpjatbcs3qzjvlumobfnugccvobqwxnnaj8.min.css" />
    
    

    
      
    
    
  </head>

  <body>
    <div id="blog">
      <header id="header" data-behavior="4">
  <i id="btn-open-sidebar" class="fa fa-lg fa-bars"></i>
  <div class="header-title">
    <a class="header-title-link" href="/">RPG</a>
  </div>
  
    
      <a class="header-right-picture "
         href="/#about">
    
    
    
      
        <img class="header-picture" src="//www.gravatar.com/avatar/9a7089fad6e7d2ebee69f9659db0c484?s=90" alt="Author&#39;s picture" />
      
    
    </a>
  
</header>

      <nav id="sidebar" data-behavior="4">
  <div class="sidebar-container">
    
      <div class="sidebar-profile">
        <a href="/#about">
          <img class="sidebar-profile-picture" src="//www.gravatar.com/avatar/9a7089fad6e7d2ebee69f9659db0c484?s=110" alt="Author&#39;s picture" />
        </a>
        <h4 class="sidebar-profile-name">孟祥良</h4>
        
          <h5 class="sidebar-profile-bio">R语言爱好者, 心理学专业硕士 &amp; FGO休闲玩家</h5>
        
      </div>
    
    <ul class="sidebar-buttons">
      
  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="/">
    
      <i class="sidebar-button-icon fa fa-lg fa-home"></i>
      
      <span class="sidebar-button-desc">Home</span>
    </a>
  </li>

  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="/categories">
    
      <i class="sidebar-button-icon fa fa-lg fa-bookmark"></i>
      
      <span class="sidebar-button-desc">Categories</span>
    </a>
  </li>

  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="/tags">
    
      <i class="sidebar-button-icon fa fa-lg fa-tags"></i>
      
      <span class="sidebar-button-desc">Tags</span>
    </a>
  </li>

  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="/archives">
    
      <i class="sidebar-button-icon fa fa-lg fa-archive"></i>
      
      <span class="sidebar-button-desc">Archives</span>
    </a>
  </li>

  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="/#about">
    
      <i class="sidebar-button-icon fa fa-lg fa-question"></i>
      
      <span class="sidebar-button-desc">About</span>
    </a>
  </li>


    </ul>
    <ul class="sidebar-buttons">
      
  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://github.com/rrtbmxl/rrtbmxl.github.io">
    
      <i class="sidebar-button-icon fa fa-lg fa-github"></i>
      
      <span class="sidebar-button-desc">GitHub</span>
    </a>
  </li>

  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="/">
    
      
      
      <span class="sidebar-button-desc"></span>
    </a>
  </li>


    </ul>
    <ul class="sidebar-buttons">
      
  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="/">
    
      
      
      <span class="sidebar-button-desc"></span>
    </a>
  </li>


    </ul>
  </div>
</nav>

      

      <div id="main" data-behavior="4"
        class="
               hasCoverMetaIn
               ">
        <article class="post" itemscope itemType="http://schema.org/BlogPosting">
          
          
            <div class="post-header main-content-wrap text-left">
  
    <h1 class="post-title" itemprop="headline">
      使用R语言文本挖掘技术应对英语考试的粗浅想法及其他
    </h1>
  
  
  <div class="postShorten-meta post-meta">
    
      <time itemprop="datePublished" datetime="2018-03-15T00:00:00Z">
        
  March 15, 2018

      </time>
    
    
  
  
    <span>in</span>
    
      <a class="category-link" href="/categories/r">R</a>
    
  

  </div>

</div>
          
          <div class="post-content markdown" itemprop="articleBody">
            <div class="main-content-wrap">
              


<div class="section level2">
<h2>学习与考试</h2>
<p>学习是由经验引起的相对持久的变化，而考试，一方面是对习得的知识与技能的掌握情况的测试，另一方面也是对能否进入下一阶段学习或工作的考核。虽然学习能力受个人的智力水平以及学习方法的影响，但一般而言，学习是没有捷径的；但考试却不一样，在应付考试的时候，确实存在一些捷径可走。且以我个人的经历举三个例子。</p>
<p>高中的时候，我的立体几何学得不是很好，对于如何解立体几何大题完全摸不着门路。然而有一天，我在一本辅导书中看到一种用空间向量解立体几何大题的方法，而且相当简单易学，做了几道类似的题，我就掌握了这个方法。随后在整个高中阶段，我就靠着这一捷径，在并不具备足够的立体几何知识的前提下做到了立体几何大题从来不丢分。</p>
<p>我本科是自考的英语专业，其中有一门课，叫《英美文学选读》。这门课应该是所有20多门课中最有价值的一门，同时也是最难的一门。教材中介绍了英国与美国的70多位著名的作家以及他们的代表作。我第一次没能考过，因为内容实在是太多了，不好把握重点。后来，我找了几份历年的真题，看过之后才发现，原来70多位作者当中，只有30多位会在考试中考到，这一下就把任务量缩减了一半。虽然难度还是挺大的，但第二次终于顺便通过了。我又一次找到了考试的捷径，同时也第一次意识到真题对于考试的重要性。</p>
<p>拿到本科毕业证之后，我就开始着手考研。专业课所涉及的内容不可谓不多，如果想全面的复习，时间肯定不够，只能有所侧重的进行复习。有了先前的经验，我开始反复地研究真题。哪些知识点会以选择题的形式考察，哪些知识点会以大题的形式考察，哪些部分的内容考试从来不涉及，可以忽略，哪些部分的内容频繁考到，必须重点记忆？经过反复地分析，我根据真题总结出了一份比较精简的笔记，并在考试中拿到了一个还算满意的分数。如果仅仅按大纲来复习的话，肯定会浪费很多时间。</p>
<p>从我个人经历来看，考试确实是有捷径可循，而这个捷径往往跟历年的真题有关。但是对于大部分考试来说，可用于分析的样本量太低，而且找不到客观的指标，只能通过自己的主观判断，因此在知识点的选取上难免会有一些偏差。但有一门考试却不存在这两个问题，既有足够大的样本量可供分析，又能产生客观的指标。很明显，这门考试就是大家“喜闻乐见”英语考试。</p>
<p>上大学之前，语数外三科同等重要；而上大学之后，大多数专业应该都不用学语文了，文科的一般也不用学数学了，只有英语，坚挺地为几乎所有专业所必学，并在很多学校作为毕业的条件之一。从这一点也可以看出来英语的重要性。学好英语不是一件轻松的事情，需要长时间的积累。虽然我恐怕不会再参加任何类型的英语考试了，但是未了将来的更好发展，我基本上保持着每天10K的英文阅读量。不过，应付英语考试就是另外一回事了，完全有可能出现我在备考《英美文学选读》时的情况。此外，我还想强调一点，<strong>我们这一辈子为了应付考试浪费了太多的生命了，也许功利地应付考试，反而倒是一种非功利的学习行为。</strong></p>
<p>要准备英语考试，大部分人的第一反应应该都是要<font size = 6> 背单词！</font>各个级别的英语考试都有那么几千到上万的单词需要背诵，绝大多数考生都或多或少地受到过背单词的折磨。背了忘，忘了再背，背了再忘，反反复复，没有休止。一方面，对单词的记忆会随着时间的流逝而消退，另一方面，几千个单词之间往往也会产生各种干扰，导致遗忘。如果需要背诵的单词量大大减少的话，情况就会好得多了。那现在的问题就是，各级别英语考试大纲中的那些单词，真的都需要背下来吗？</p>
<p>社会学中有个著名的80/20定律，即这个世界80%的财富都集中在20%的人手里。对于英语试卷这一定律可能也适用，即80%的文本都是由那20%最常见的单词组成。如果真是这样的话，那只需要原先五分之一的时间（也许更少，因为这时的干扰也更少），就足以应付英语考试了。市面上其实早就有了通过词频来排列单词供人背诵的参考书，但是这些书的销量似乎并不是很好，这大概是因为这些书只是笼统的把全部文本都拿来进行统计，不够细致，也没有针对性。如果能针对不同类型的试题，甚至针对这些试题的不同部分，统计出来词频，肯定会有更好的指导作用。</p>
</div>
<div class="section level2">
<h2>针对高考英语阅读题的文本分析</h2>
<p>为了进行验证与分析，我从网上找到了最近9年的天津高考英语试题文档。我把其中的阅读部分复制了出来，按年份保存在了txt文档中，然后利用<code>tidyverse</code>和<code>tidytext</code>包，制作了两份csv格式句子库文档，其中一份是以全部阅读文本为基础生成的，另一份则只针对阅读题的题干而生成。现在先载入需要的包，并把这两份数据导入：</p>
<pre class="r"><code>library(tidyverse)
library(tidytext)
library(pipeR)
library(magrittr)
library(knitr)

reading &lt;- read.csv(&#39;reading.csv&#39;, stringsAsFactors = FALSE)
question &lt;- read.csv(&#39;question.csv&#39;, stringsAsFactors = FALSE)</code></pre>
<p>因为<code>readr</code>包中的函数对中文的支持不是很好，所以我这里使用了<code>R</code>自带的函数。先看一下这两份数据：</p>
<pre class="r"><code>reading %&gt;% head() %&gt;% kable()</code></pre>
<table>
<thead>
<tr class="header">
<th align="right">year</th>
<th align="left">region</th>
<th align="left">passage</th>
<th align="left">sentence</th>
<th align="left">sentence_low</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">2007</td>
<td align="left">天津</td>
<td align="left">A</td>
<td align="left">The city of Rome has passed a new law to prevent cruelty to animals.</td>
<td align="left">the city of rome has passed a new law to prevent cruelty to animals.</td>
</tr>
<tr class="even">
<td align="right">2007</td>
<td align="left">天津</td>
<td align="left">A</td>
<td align="left">All goldfish bowls are no longer allowed and dog owners must walk their dogs.</td>
<td align="left">all goldfish bowls are no longer allowed and dog owners must walk their dogs.</td>
</tr>
<tr class="odd">
<td align="right">2007</td>
<td align="left">天津</td>
<td align="left">A</td>
<td align="left">This comes after a national law was passed to give prison sentences to people who desert cats or dogs.</td>
<td align="left">this comes after a national law was passed to give prison sentences to people who desert cats or dogs.</td>
</tr>
<tr class="even">
<td align="right">2007</td>
<td align="left">天津</td>
<td align="left">A</td>
<td align="left">“The civilization of a city can be measured by this,” said Monica Carina, the councilor (议员) behind the new law.</td>
<td align="left">“the civilization of a city can be measured by this,” said monica carina, the councilor (议员) behind the new law.</td>
</tr>
<tr class="odd">
<td align="right">2007</td>
<td align="left">天津</td>
<td align="left">A</td>
<td align="left">“It’s good to do whatever we can for our animals who in exchange for a little love fill our existence with their attention,” she told a Rome newspaper.</td>
<td align="left">“it’s good to do whatever we can for our animals who in exchange for a little love fill our existence with their attention,” she told a rome newspaper.</td>
</tr>
<tr class="even">
<td align="right">2007</td>
<td align="left">天津</td>
<td align="left">A</td>
<td align="left">The newspaper reported that round bowls don’t give enough oxygen for fish and may make them go blind.</td>
<td align="left">the newspaper reported that round bowls don’t give enough oxygen for fish and may make them go blind.</td>
</tr>
</tbody>
</table>
<pre class="r"><code>question %&gt;% head() %&gt;% kable()</code></pre>
<table>
<thead>
<tr class="header">
<th align="right">year</th>
<th align="left">region</th>
<th align="left">passage</th>
<th align="right">number</th>
<th align="left">sentence</th>
<th align="left">sentence_low</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">2007</td>
<td align="left">天津</td>
<td align="left">A</td>
<td align="right">36</td>
<td align="left">The new law passed in Rome will ______.</td>
<td align="left">the new law passed in rome will ______.</td>
</tr>
<tr class="even">
<td align="right">2007</td>
<td align="left">天津</td>
<td align="left">A</td>
<td align="right">37</td>
<td align="left">People in Rome believe that the civilization of a city can be judged by its ______.</td>
<td align="left">people in rome believe that the civilization of a city can be judged by its ______.</td>
</tr>
<tr class="odd">
<td align="right">2007</td>
<td align="left">天津</td>
<td align="left">A</td>
<td align="right">38</td>
<td align="left">The underlined word “compassion” in Paragraph 6 is the closest in meaning to ______.</td>
<td align="left">the underlined word “compassion” in paragraph 6 is the closest in meaning to ______.</td>
</tr>
<tr class="even">
<td align="right">2007</td>
<td align="left">天津</td>
<td align="left">A</td>
<td align="right">39</td>
<td align="left">People may break the law in Turin if they ______.</td>
<td align="left">people may break the law in turin if they ______.</td>
</tr>
<tr class="odd">
<td align="right">2007</td>
<td align="left">天津</td>
<td align="left">B</td>
<td align="right">40</td>
<td align="left">Charles Blackman’s paintings come from ______.</td>
<td align="left">charles blackman’s paintings come from ______.</td>
</tr>
<tr class="even">
<td align="right">2007</td>
<td align="left">天津</td>
<td align="left">B</td>
<td align="right">41</td>
<td align="left">Which two activities can you participate in on the same day?</td>
<td align="left">which two activities can you participate in on the same day?</td>
</tr>
</tbody>
</table>
<p>可以看到，全部阅读文本（<code>reading</code>）的句子库包含年份、地区、篇号、原始句子以及完全小写的句子（两种句子有不同的用处）五列变量，而阅读题干文本（<code>question</code>）的句子库还多了题号一列。</p>
<p>首先，我要利用<code>reading</code>这份数据来验证一下，是不是最常见的20%的单词组成了80%的文本。为了进行这个验证，我要先把词频统计出来，这部分统计的代码如下：</p>
<pre class="r"><code>word_freq &lt;- reading %&gt;% unnest_tokens(word, sentence) %&gt;% 
  mutate(word = str_replace_all(word, &quot;(_)|(\\d)|(’s)|([abcd]{1}\\.)&quot;, &quot;&quot;)) %&gt;%  
  count(word) %&gt;%
  filter(nchar(word) &gt; 2) %&gt;% 
  arrange(-n) %&gt;% 
  mutate(num = 1:nrow(.))</code></pre>
<p>看一下：</p>
<pre class="r"><code>str(word_freq)</code></pre>
<pre><code>## Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;:    3760 obs. of  3 variables:
##  $ word: chr  &quot;the&quot; &quot;and&quot; &quot;for&quot; &quot;that&quot; ...
##  $ n   : int  1260 488 213 200 152 151 149 138 125 125 ...
##  $ num : int  1 2 3 4 5 6 7 8 9 10 ...</code></pre>
<p>可以看到，一共有3760个单词（实际上还有一些非英语单词字符未清理，但这个数不会差太多），乘以0.2也就是大概752个，下面分别计算前20%的单词组成了多少文本以及共有多少文本：</p>
<pre class="r"><code>word_freq %&gt;% filter(num &lt; 660) %$% sum(n)</code></pre>
<pre><code>## [1] 12282</code></pre>
<pre class="r"><code>sum(word_freq$n)</code></pre>
<pre><code>## [1] 17820</code></pre>
<p>分别是12282和17820，除一下，可以发现，大概是70%左右，虽然没有达到80%，但也没差太多。我看了一下词频表的具体情况，发现排名660的单词频次为4，为了覆盖更多的文本，我决定多加入一些词，把频次为3的单词也包括进来，这些词共组成了14668字的文本，差不多已经达到了80%，不过这时的单词量有点多，已经达到了1324个，还需要进一步的筛除。</p>
<p>去年刚接触文本分析的时候，用天津2016年高考英语试题画了个词汇云，有点意外的是，<strong>fatigue</strong>这个单词出现的频率极高，但这个词应该不是高中阶段需要掌握的，它可能仅仅出现在这一年的某一篇文章中，对此，我验证了一下：</p>
<pre class="r"><code>filter(reading, str_detect(sentence_low, &#39;fatigue&#39;)) %&gt;% kable()</code></pre>
<table>
<thead>
<tr class="header">
<th align="right">year</th>
<th align="left">region</th>
<th align="left">passage</th>
<th align="left">sentence</th>
<th align="left">sentence_low</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">2016</td>
<td align="left">天津</td>
<td align="left">D</td>
<td align="left">We experience this tiredness in two ways: as start-up fatigue (疲惫) and performance fatigue.</td>
<td align="left">we experience this tiredness in two ways: as start-up fatigue (疲惫) and performance fatigue.</td>
</tr>
<tr class="even">
<td align="right">2016</td>
<td align="left">天津</td>
<td align="left">D</td>
<td align="left">Such start-up fatigue is very real, even if not actually physical, not something in our muscles and bones.</td>
<td align="left">such start-up fatigue is very real, even if not actually physical, not something in our muscles and bones.</td>
</tr>
<tr class="odd">
<td align="right">2016</td>
<td align="left">天津</td>
<td align="left">D</td>
<td align="left">Performance fatigue is more difficult to handle.</td>
<td align="left">performance fatigue is more difficult to handle.</td>
</tr>
<tr class="even">
<td align="right">2016</td>
<td align="left">天津</td>
<td align="left">D</td>
<td align="left">My fatigue became almost unbearable.</td>
<td align="left">my fatigue became almost unbearable.</td>
</tr>
<tr class="odd">
<td align="right">2016</td>
<td align="left">天津</td>
<td align="left">D</td>
<td align="left">Though I worked as hard as before, I felt no fatigue.</td>
<td align="left">though i worked as hard as before, i felt no fatigue.</td>
</tr>
<tr class="even">
<td align="right">2016</td>
<td align="left">天津</td>
<td align="left">D</td>
<td align="left">51 People with start-up fatigue are most likely to ______.</td>
<td align="left">51 people with start-up fatigue are most likely to ______.</td>
</tr>
<tr class="odd">
<td align="right">2016</td>
<td align="left">天津</td>
<td align="left">D</td>
<td align="left">52 What does the author recommend doing to prevent start-up fatigue?</td>
<td align="left">52 what does the author recommend doing to prevent start-up fatigue?</td>
</tr>
<tr class="even">
<td align="right">2016</td>
<td align="left">天津</td>
<td align="left">D</td>
<td align="left">53 On what occasion does a person probably suffer from performance fatigue?</td>
<td align="left">53 on what occasion does a person probably suffer from performance fatigue?</td>
</tr>
<tr class="odd">
<td align="right">2016</td>
<td align="left">天津</td>
<td align="left">D</td>
<td align="left">B.How to Handle Performance Fatigue</td>
<td align="left">b.how to handle performance fatigue</td>
</tr>
<tr class="even">
<td align="right">2016</td>
<td align="left">天津</td>
<td align="left">D</td>
<td align="left">C.Getting over Fatigue: A Way to Success</td>
<td align="left">c.getting over fatigue: a way to success</td>
</tr>
<tr class="odd">
<td align="right">2016</td>
<td align="left">天津</td>
<td align="left">D</td>
<td align="left">D.Fatigue: An Early Sign of Health Problems</td>
<td align="left">d.fatigue: an early sign of health problems</td>
</tr>
</tbody>
</table>
<p>果然，这个单词只出现在了2016年的D篇之中，而且是以带有释义的形式出现的，这样的单词，即便出现的频率很高，也没有背诵的必要。针对这一点，我决定只把在至少在3篇文章中都出现过1次的单词筛选出来，并对代码进行了相应的修改：</p>
<pre class="r"><code>word_freq &lt;- reading %&gt;% unnest_tokens(word, sentence) %&gt;% 
  mutate(word = str_replace_all(word, &quot;(_)|(\\d)|(’s)|([abcd]{1}\\.)&quot;, &quot;&quot;)) %&gt;%
  unite(&#39;year_passage&#39;, c(year, passage)) %&gt;% 
  group_by(word) %&gt;% 
  summarise(n = n(), m = length(unique(year_passage))) %&gt;% 
  filter(nchar(word) &gt; 2) %&gt;% 
  arrange(-n) %&gt;% 
  filter(m &gt; 2)</code></pre>
<p>看一下现在的情况：</p>
<pre class="r"><code>str(word_freq)</code></pre>
<pre><code>## Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;:    921 obs. of  3 variables:
##  $ word: chr  &quot;the&quot; &quot;and&quot; &quot;for&quot; &quot;that&quot; ...
##  $ n   : int  1260 488 213 200 152 151 149 138 125 125 ...
##  $ m   : int  46 46 46 46 41 42 28 42 44 41 ...</code></pre>
<pre class="r"><code>word_freq %&gt;% head(10) %&gt;% kable()</code></pre>
<table>
<thead>
<tr class="header">
<th align="left">word</th>
<th align="right">n</th>
<th align="right">m</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">the</td>
<td align="right">1260</td>
<td align="right">46</td>
</tr>
<tr class="even">
<td align="left">and</td>
<td align="right">488</td>
<td align="right">46</td>
</tr>
<tr class="odd">
<td align="left">for</td>
<td align="right">213</td>
<td align="right">46</td>
</tr>
<tr class="even">
<td align="left">that</td>
<td align="right">200</td>
<td align="right">46</td>
</tr>
<tr class="odd">
<td align="left">are</td>
<td align="right">152</td>
<td align="right">41</td>
</tr>
<tr class="even">
<td align="left">with</td>
<td align="right">151</td>
<td align="right">42</td>
</tr>
<tr class="odd">
<td align="left">you</td>
<td align="right">149</td>
<td align="right">28</td>
</tr>
<tr class="even">
<td align="left">what</td>
<td align="right">138</td>
<td align="right">42</td>
</tr>
<tr class="odd">
<td align="left">can</td>
<td align="right">125</td>
<td align="right">44</td>
</tr>
<tr class="even">
<td align="left">from</td>
<td align="right">125</td>
<td align="right">41</td>
</tr>
</tbody>
</table>
<p>此时只剩下730个单词了。由于我一共收集了9套试卷，而每套试卷中有4段阅读，所以一共是36篇文章。可以看到，<strong>the</strong>和<strong>for</strong>这两个单词在所有的文章中都出现了，<strong>and</strong>和<strong>that</strong>等单词也几乎在每一篇文章中都出现了，但是这类单词过于简单，根本就不需要再背了，因此还需要根据经验，总结一个停用词表，以便将这类词汇排除在外。不过，我目前还没有整理这份停用词表，因为文本本身还不够干净，存在一些错误，校对后还要重新整理停用词表，所以我就暂时没弄。如果根据停用词表进行了筛除的话，那最后剩下的词汇应该只有三四百个了，这三四百个单词就是天津高考英语阅读的核心词汇了。</p>
<p>另外，像之前查找<strong>fatigue</strong>单词的出现情况一样，可以利用建好的句子库和<code>filter</code>等函数非常方便地提取包含某单词的句子，甚至直接以辅导书上例句格式的形式输出，现以包含<strong>psychology</strong>一词的句子为例，代码如下：</p>
<pre class="r"><code>filter(reading, str_detect(sentence, &#39;psychology&#39;)) %$% 
  str_c(sentence[2], &#39;(&#39;, region[2], &#39;, &#39;, year[2], passage[2], &#39;)&#39;)</code></pre>
<pre><code>## [1] &quot;Virginia Berninger, professor of educational psychology at the University of Washington, says it&#39;s important to continue teaching handwriting and help children acquire the skill of writing by hand.(天津, 2012C)&quot;</code></pre>
<p>那些辅导教材的编写者应该都有这样一个句子库，否则编写教材将会变成一个非常耗时的工作。我可以慢慢地以其他文本（如新概念之类）为依托建立新的句子库，方便以后编写教案（如果有必要的话）。另外，虽然话是这么说，但我是真的讨厌用手写字…</p>
<p>以上是根据全部阅读文本所作的分析，另外，我还单独把阅读题的题干提取出来，进行了类似的分析。根据我个人的学习经历和辅导他人的经历，发现很多时候，之所以做不对一道阅读题，是因为连题干都读不懂，因此很有必要对那些经常在题干中出现的词进行重点掌握。另外，也可以通过这种方式了解道各种类型的问题的分布情况。</p>
</div>
<div class="section level2">
<h2>局限与展望</h2>
<p>虽然名为文本挖掘，实际上根本就未涉及到太深入的内容。但话说回来，像情感分析、主题模型之类的分析在这里也用不上。不过，n-gram技术倒是值得尝试。我还没见过哪本单词书把常用词组的频次排出来呢，我可以试着把它们找出来。</p>
<p>另外的问题是文本量太少，而且还需要进一步的校对。虽然我下载的那些文档的文件名上都标明了是“精校版”，但实际上都还存在很多错误，我还要自己校正一下。然后文本量的问题，因为我现在只找了天津的高考试题，所以比较少，如果把其他省份的试题也都加进来的话，文本量应该能有十几万，这样得出的结果会更可靠一些。然而，这也带来了新的问题，即不同地区的试卷能否互作参考？北京卷能用来指导天津的考试吗？不同地区的试卷似乎还要做个相关来验证下这个问题。</p>
<p>还有一个不知道该如何解决的问题，即名词的复数与动词的时态问题。book和books会被解析为两个单词，这势必会影响单词的真正频次，不过，就目前的观察来看，影响也没那么大。将来可以考虑建立若干词库来解决这一问题。</p>
<p>我这里只针对阅读题型进行了分析，其他题型也可以进行类似的分析。比如完形，可以考虑将其分为原始文本和空白填充后的文本，对于前者查看单词的频次，而对于后者，则查看词汇之间的关系，来寻找那些最重要的词组。在完形与阅读里所涉及的单词都是消极词汇，只需要认识和理解即可，而要能写出优秀的作文，还需要掌握一部分能熟练使用的积极词汇。对优秀范文的文本分析将可以定位出重要的积极词汇，并查看这些词汇是如何被使用的，从而对写作作出指导。前段时间看了篇文章，说道有人利用机器学习方法对《冰与火之歌》的文本进行了训练，然后写出续作……看来只要有足够的文本，完全也可以利用机器学习帮自己写出作文模板。我认为任务驱动的学习是最好的学习，我可以尝试着去实现一下这个想法，并通过这个过程学习机器学习的相关技术。</p>
<p>虽然我选用的是高考的试题，但显然这也可以应用到其他级别的考试，只要有相应的试题文本就可以了。但是，不同级别的考试（以及不同的题型）需要有不同的停用词库，高考中非常重要的一个词汇可能仅仅是考研词汇中的一个停用词。另外，我还在想，这个技术是不是也可以用于文献阅读？收集个一百来篇领域内的英文文献进行分析，挑出消极词汇和积极词汇，重要的消极词汇牢记在心，重要的积极词汇掌握用法，肯定对阅读文献和写作论文有很大的帮助。</p>
</div>
<div class="section level2">
<h2>牢骚</h2>
<p>既然提到了写论文，下面将进入牢骚时间。硕士三年之间，我一共投了两次文章，一次SCI，一次SSCI；一次直接被拒，一次大修。后面这次时间赶的不是很好，刚接到通知，孩子就病了，等孩子好了，眼看就过年了，期间各种干扰，实在无法静下心来改文章；更关键的是，我虽然想考博士，但根本就没人要，既然读不了博，发不发文章也就没啥意义了，于是就这么不了了之了（最近开始学python了，对于不能研究<strong>pychology</strong>，甚感遗憾）。两年的心血，就这么付诸东流了啊，真是不甘心，不过，从另一个角度来看，也算是一种解脱。</p>
<p>出于对声优水濑祈的喜爱，前段时间补了个动画，叫《少女终末旅行》。动画讲述了两位少女在战后的无人区中旅行的见闻。虽说是无人区，但她们还是遇到了两个人，一个是男性地图绘制者，一个是女性飞机制造者。后者给我的触动很大。她独自一人制造着飞机，为了造好飞机之后，能飞向有人居住的地方。遇到两位女主人公的时候，飞机已接近完成，三人齐心合力，完成了最后的工作。然而，在真正飞行的时候，她的飞机没有飞出多远，就坠毁了。降落伞下的她缓缓落下，脸上带着满足的笑容。见到这一情景的女主人公之一大为不解，但随后，她就给出了自己的推测：</p>
<div class="figure">
<img src="/img/少女终末旅行06_1.bmp" />

</div>
<p>–</p>
<div class="figure">
<img src="/img/少女终末旅行06_2.bmp" />

</div>
<p>–</p>
<p>笑容.jpg</p>
</div>

              
            </div>
          </div>
          <div id="post-footer" class="post-footer main-content-wrap">
            
              
                
                
                  <div class="post-footer-tags">
                    <span class="text-color-light text-small">TAGGED IN</span><br/>
                    
  <a class="tag tag--primary tag--small" href="/tags/%E6%95%99%E8%82%B2/">教育</a>

                  </div>
                
              
            
            
  <div class="post-actions-wrap">
      <nav >
        <ul class="post-actions post-action-nav">
          
            <li class="post-action">
              
                <a class="post-action-btn btn btn--default tooltip--top" href="/2018/10/%E6%97%A5%E8%AE%B0%E6%96%87%E6%9C%AC%E5%88%86%E6%9E%90-%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86/" data-tooltip="日记文本分析：第一部分">
              
                  <i class="fa fa-angle-left"></i>
                  <span class="hide-xs hide-sm text-small icon-ml">NEXT</span>
                </a>
            </li>
            <li class="post-action">
              
                <a class="post-action-btn btn btn--default tooltip--top" href="/2018/02/stereotype-on-anime/" data-tooltip="关于动画的刻板印象">
              
                  <span class="hide-xs hide-sm text-small icon-mr">PREVIOUS</span>
                  <i class="fa fa-angle-right"></i>
                </a>
            </li>
          
        </ul>
      </nav>
    <ul class="post-actions post-action-share" >
      
        <li class="post-action hide-lg hide-md hide-sm">
          <a class="post-action-btn btn btn--default btn-open-shareoptions" href="#btn-open-shareoptions">
            <i class="fa fa-share-alt"></i>
          </a>
        </li>
        
          <li class="post-action hide-xs">
            <a class="post-action-btn btn btn--default" target="new" href="https://www.facebook.com/sharer/sharer.php?u=/2018/03/tackle-english-exam-with-text-mining/">
              <i class="fa fa-facebook-official"></i>
            </a>
          </li>
        
          <li class="post-action hide-xs">
            <a class="post-action-btn btn btn--default" target="new" href="https://twitter.com/intent/tweet?text=/2018/03/tackle-english-exam-with-text-mining/">
              <i class="fa fa-twitter"></i>
            </a>
          </li>
        
          <li class="post-action hide-xs">
            <a class="post-action-btn btn btn--default" target="new" href="https://plus.google.com/share?url=/2018/03/tackle-english-exam-with-text-mining/">
              <i class="fa fa-google-plus"></i>
            </a>
          </li>
        
      
      
        <li class="post-action">
          <a class="post-action-btn btn btn--default" href="#disqus_thread">
            <i class="fa fa-comment-o"></i>
          </a>
        </li>
      
      <li class="post-action">
        
          <a class="post-action-btn btn btn--default" href="#">
        
          <i class="fa fa-list"></i>
        </a>
      </li>
    </ul>
  </div>


            
              
                <div id="disqus_thread">
  <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>
              
            
          </div>
        </article>
        <footer id="footer" class="main-content-wrap">
  <span class="copyrights">
    &copy; 2019 孟祥良. All Rights Reserved
  </span>
</footer>

      </div>
      <div id="bottom-bar" class="post-bottom-bar" data-behavior="4">
        
  <div class="post-actions-wrap">
      <nav >
        <ul class="post-actions post-action-nav">
          
            <li class="post-action">
              
                <a class="post-action-btn btn btn--default tooltip--top" href="/2018/10/%E6%97%A5%E8%AE%B0%E6%96%87%E6%9C%AC%E5%88%86%E6%9E%90-%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86/" data-tooltip="日记文本分析：第一部分">
              
                  <i class="fa fa-angle-left"></i>
                  <span class="hide-xs hide-sm text-small icon-ml">NEXT</span>
                </a>
            </li>
            <li class="post-action">
              
                <a class="post-action-btn btn btn--default tooltip--top" href="/2018/02/stereotype-on-anime/" data-tooltip="关于动画的刻板印象">
              
                  <span class="hide-xs hide-sm text-small icon-mr">PREVIOUS</span>
                  <i class="fa fa-angle-right"></i>
                </a>
            </li>
          
        </ul>
      </nav>
    <ul class="post-actions post-action-share" >
      
        <li class="post-action hide-lg hide-md hide-sm">
          <a class="post-action-btn btn btn--default btn-open-shareoptions" href="#btn-open-shareoptions">
            <i class="fa fa-share-alt"></i>
          </a>
        </li>
        
          <li class="post-action hide-xs">
            <a class="post-action-btn btn btn--default" target="new" href="https://www.facebook.com/sharer/sharer.php?u=/2018/03/tackle-english-exam-with-text-mining/">
              <i class="fa fa-facebook-official"></i>
            </a>
          </li>
        
          <li class="post-action hide-xs">
            <a class="post-action-btn btn btn--default" target="new" href="https://twitter.com/intent/tweet?text=/2018/03/tackle-english-exam-with-text-mining/">
              <i class="fa fa-twitter"></i>
            </a>
          </li>
        
          <li class="post-action hide-xs">
            <a class="post-action-btn btn btn--default" target="new" href="https://plus.google.com/share?url=/2018/03/tackle-english-exam-with-text-mining/">
              <i class="fa fa-google-plus"></i>
            </a>
          </li>
        
      
      
        <li class="post-action">
          <a class="post-action-btn btn btn--default" href="#disqus_thread">
            <i class="fa fa-comment-o"></i>
          </a>
        </li>
      
      <li class="post-action">
        
          <a class="post-action-btn btn btn--default" href="#">
        
          <i class="fa fa-list"></i>
        </a>
      </li>
    </ul>
  </div>


      </div>
      <div id="share-options-bar" class="share-options-bar" data-behavior="4">
  <i id="btn-close-shareoptions" class="fa fa-close"></i>
  <ul class="share-options">
    
      <li class="share-option">
        <a class="share-option-btn" target="new" href="https://www.facebook.com/sharer/sharer.php?u=%2F2018%2F03%2Ftackle-english-exam-with-text-mining%2F">
          <i class="fa fa-facebook-official"></i><span>Share on Facebook</span>
        </a>
      </li>
    
      <li class="share-option">
        <a class="share-option-btn" target="new" href="https://twitter.com/intent/tweet?text=%2F2018%2F03%2Ftackle-english-exam-with-text-mining%2F">
          <i class="fa fa-twitter"></i><span>Share on Twitter</span>
        </a>
      </li>
    
      <li class="share-option">
        <a class="share-option-btn" target="new" href="https://plus.google.com/share?url=%2F2018%2F03%2Ftackle-english-exam-with-text-mining%2F">
          <i class="fa fa-google-plus"></i><span>Share on Google&#43;</span>
        </a>
      </li>
    
  </ul>
</div>
<div id="share-options-mask" class="share-options-mask"></div>
    </div>
    
    <div id="about">
  <div id="about-card">
    <div id="about-btn-close">
      <i class="fa fa-remove"></i>
    </div>
    
      <img id="about-card-picture" src="//www.gravatar.com/avatar/9a7089fad6e7d2ebee69f9659db0c484?s=110" alt="Author&#39;s picture" />
    
    <h4 id="about-card-name">孟祥良</h4>
    
      <div id="about-card-bio">R语言爱好者, 心理学专业硕士 &amp; FGO休闲玩家</div>
    
    
    
  </div>
</div>

    <div id="algolia-search-modal" class="modal-container">
  <div class="modal">
    <div class="modal-header">
      <span class="close-button"><i class="fa fa-close"></i></span>
      <a href="https://algolia.com" target="_blank" rel="noopener" class="searchby-algolia text-color-light link-unstyled">
        <span class="searchby-algolia-text text-color-light text-small">by</span>
        <img class="searchby-algolia-logo" src="https://www.algolia.com/static_assets/images/press/downloads/algolia-light.svg">
      </a>
      <i class="search-icon fa fa-search"></i>
      <form id="algolia-search-form">
        <input type="text" id="algolia-search-input" name="search"
          class="form-control input--large search-input" placeholder="Search" />
      </form>
    </div>
    <div class="modal-body">
      <div class="no-result text-color-light text-center">no post found</div>
      <div class="results">
        
        
          <div class="media">
            
            <div class="media-body">
              <a class="link-unstyled" href="/2019/04/r-resources-collection/">
                <h3 class="media-heading">R语言学习资源总结</h3>
              </a>
              <span class="media-meta">
                <span class="media-date text-small">
                  Apr 4, 2019
                </span>
              </span>
              <div class="media-content hide-xs font-merryweather">ArticleAdvantageConceptHistoryLearningCommunicationBlogdownReportShinyWordMachine LearningBasicsClassificationClusteringDeep LearningDimensionality ReductionPredictionProgramEvaluationFunctionLoopPipeTutorialData AcquisitionOnlinebookTidyverseWebsiteVisualizationAdjustmentBasicCommon PlotsColorExtensionLine ChartMapScatter PlotTheory总结一下我自己看过的R语言学习资源，目前的分类还比较随意，因为很多内容都是交叉的，难以形成明确的体系。没啥特殊情况的话，每个周末都会补充新的内容。另外要强调一点，看教程虽然有助于学习，但真正能提升水平的，还是实践。如果工作中没有太多实践的机会的话，那也要自己想办法找些实践的机会。
还要吐槽一下，不论是tidyverse还是Rmarkdown，对中文的支持都不是很好，用中文标题生成的TOC无法跳转，所以我只好暂时把各部分的标题都写成英文了，以后有时间再琢磨这个问题。不过话说回来，确实是写成英文更方便点，因为我收藏夹里的文件夹名都是英文命名的，而这篇博客其实就是对我的收藏夹内容的总结。
Article这里罗列了一些关于R语言的文章，主要包括R的优势、与R相关的一些概念、R的历史以及如何学习R。
Advantage这一部分收集了一些介绍R语言优势的文章。
Why I use R for Data Science - An Ode to R (181110; 190306)</div>
            </div>
            <div style="clear:both;"></div>
            <hr>
          </div>
        
          <div class="media">
            
            <div class="media-body">
              <a class="link-unstyled" href="/2019/04/r-notebook-and-plan/">
                <h3 class="media-heading">R学习笔记及学习计划</h3>
              </a>
              <span class="media-meta">
                <span class="media-date text-small">
                  Apr 4, 2019
                </span>
              </span>
              <div class="media-content hide-xs font-merryweather">人对事物的认识大概可以分为三个层次，从“未知的未知”到“已知的未知”再到“已知的已知”。如果确实是这样的话，那学习就可以分为两种，一种是把“未知的未知”变为“已知的未知”，如了解到这个世界上存在一种叫做“负数”的东西，但不知道它究竟指什么；另一种是把“已知的未知”变为“已知的已知”，如通过进一步的了解，获知“负数”的确切意义。德尔菲的神谕认为没有人比苏格拉底更聪明，其看重的可能并不在于苏格拉底是否比其他所有人拥有更多“已知的已知”，而是看到他比其他人拥有更多“已知的未知”。我不知道两种学习中哪一种更为重要，但我觉得，在大多数情况下，前一种学习都是后一种学习的先决条件。人的时间是有限的，没法把所有的知识都掌握，所以比较好的学习思路可能是先去获取足够多的“已知的未知”，再决定把哪些“已知的未知”转变为“已知的已知”。
我接触R已经三年多了，但真正开始学习R，也就一年多的样子。我对R本身其实没有多大的兴趣，但当我把tidyverse变为“已知的未知”时，才对这门语言产生了热情。翻开哲学的入门书，很有可能会发现最开始的章节是以苏格拉底来划分的，如类似“前苏格拉底时代的哲学家们”的说法。在这里，我也想用tidyverse这个词来对我的笔记章节进行划分（当然，tidyverse对应的哲学家更有可能是笛卡尔），具体来说，包括tidyverse之前，用来介绍R的一些基本知识；tidyverse之内，用来介绍tidyverse核心包的使用方法；tidyverse之上，用来介绍建立在tidyverse核心包基础上的一些实用的包；tidyverse之外，用来介绍与tidyverse无关，但很有用的一些包。当然，这些内容中的很大一部分对我来说还是“已知的未知”。
想弄这么个东西，目的主要有两个：一方面，把自己会的东西以教程的形式写出来，能让自己把“已知的已知”掌握得更牢固；另一方面，也能督促自己不断地去学习新知识，探索“未知的未知”，转化“已知的未知”。因此，内容方面，就包括我目前会的，和我将来想学的，具体内容可以看后面暂定的大纲。另外，我也给自己设定了几个要求：
术语尽量给出参考资料和对应的英文，不知道该如何翻译的直接用英文，符号给出对应的英文及其读音；
尽量保证所有的内容都能跟上R本体和所涉及的包的更新；
暂定的提纲如下：
tidyverse之前R的介绍及安装
R的基本概念及操作
R中的条件与循环
tidyveRse之内使用readr导入数据
使用rvest获取网络数据
dbplyr与数据库
dplyr包常用操作及管道操作符
tidyr包常用操作及tidy data
stringr包常用操作及正则表达式
forcats常用操作
lubridate常用操作
purrr包探索
组合使用
tidyverse代码风格
ggplot2基本统计图的绘制
ggplot2统计图的调整
ggplot2统计图的美化
tidyverse之上使用tidytext进行文本分析
使用ggvis绘制交互统计图
使用gganimate绘制动态统计图
tidyverse之外使用rmarkdown撰写报告
使用blogdown搭建博客
使用shiny制作网络应用</div>
            </div>
            <div style="clear:both;"></div>
            <hr>
          </div>
        
          <div class="media">
            
            <div class="media-body">
              <a class="link-unstyled" href="/2019/03/r-basic-concept-and-operation/">
                <h3 class="media-heading">R的基本概念和操作</h3>
              </a>
              <span class="media-meta">
                <span class="media-date text-small">
                  Mar 3, 2019
                </span>
              </span>
              <div class="media-content hide-xs font-merryweather">R的基本操作a.计算b.逻辑运算c.赋值R的基本概念a.数据类型b.函数c.包190323
R的基本操作a.计算R可以作为计算器使用，+、-、*、/、^分别代表加减乘除和乘方：
2 ^ 2 / 2 - (2 * 2 + 2)## [1] -4%%求余数，%/%求商：
5 %% 2## [1] 15 %/% 2## [1] 2b.逻辑运算==、!=、&gt;、&gt;=、&lt;、&lt;=分别用来判断相等、不等、大于、大于等于、小于、小于等于的关系，符合逻辑返回TRUE，反之返回FALSE。对于部分字符（英文字母和汉字），似乎是字母顺序排在后面的更大；对于字符型数值，似乎与其数值型数值相等；另外，逻辑型数值中，TRUE等于1，而FALSE等于0：
TRUE == 1## [1] TRUEFALSE == 0## [1] TRUE&#39;白马&#39; != &#39;马&#39;## [1] TRUE1 == &#39;1&#39;## [1] TRUE&#39;x&#39; &lt; &#39;y&#39;## [1] TRUE&#39;一&#39; &gt; &#39;二&#39;## [1] TRUEc.</div>
            </div>
            <div style="clear:both;"></div>
            <hr>
          </div>
        
          <div class="media">
            
            <div class="media-body">
              <a class="link-unstyled" href="/2019/03/r-introduction-and-installation/">
                <h3 class="media-heading">R的介绍和安装</h3>
              </a>
              <span class="media-meta">
                <span class="media-date text-small">
                  Mar 3, 2019
                </span>
              </span>
              <div class="media-content hide-xs font-merryweather">R的介绍R的简介R的优势R的安装RStudio的安装RStudio的设置RStudio的布局190320
R的介绍R的简介R是一种编程语言，是可以用来进行统计计算和绘图的免费软件。
R的优势免费。虽然很多付费软件也并没有真的付费去用，但用R至少不用承受负罪感了。
跨平台，Windows、linux和Mac都可以用。不过我只用过Windows，所以后面的安装教程也可能仅适用于Windows。
有近14000个包（这是19年3月20日的数量。如果没记错的话，《R语言实战》第一版上这个数量是4000，这几年增加了将近10000个包）。
有非常高端的绘图功能。这里主要指的是ggplot2包及其扩展。
有非常强大的社区支持。遇到问题可以去StackOverflow，RStudio Community等地方寻找，不过像我这么内向的人，一般就打几个关键词，然后必应一下，基本上就能找到解决办法了。
有RStudio这个非常好用的IDE（Integrated Development Environment，集成开发环境）
R的安装进入CRAN;
点左上角图标下的镜像，也就是Mirrors；
寻找离自己近的镜像，我一般就用China下的第一个，清华的；
根据自己的电脑系统点击不同的Download R for xxxxxx；
点击install R for the first time，然后就会开始下载了；
把下载好的安装包装上，所有选项都选默认吧。（换了新电脑后，只有一个C盘了，不存在想把程序装到别处的问题了）
R已经装完了，但想要用的舒服还需要把RStudio装上。
RStudio的安装点这个链接；
点适合自己电脑系统的安装包，然后就会开始下载了；
把安装包装上，同样，还是都选默认选项吧。最好把安装路径记下来，因为RStudio似乎不会自动生成桌面的快捷方式，需要你自己去把快捷方式弄到桌面上。</div>
            </div>
            <div style="clear:both;"></div>
            <hr>
          </div>
        
          <div class="media">
            
            <div class="media-body">
              <a class="link-unstyled" href="/2019/02/reading-records-analysis/">
                <h3 class="media-heading">11年-19年读书记录分析</h3>
              </a>
              <span class="media-meta">
                <span class="media-date text-small">
                  Feb 2, 2019
                </span>
              </span>
              <div class="media-content hide-xs font-merryweather">大概在10年前后，镇上给村里弄了个农村书屋，其他办公室都没地方，就把书连书柜都放我办公室里了。当时统计了下，一共有1500多本书，后来又给了一批，最终达到1800多本。这些书中的大部分质量和内容都很一般，但还是有几本好书的。反正工作也挺闲的，每天就靠看书打发点时间。时过境迁，工作已经换了几个，但看书的习惯还一直保持着。
我从12年开始记日记，所以从那一年起，哪段时间看了哪本书都有记录。前几年把读书的记录整理过一次，但信息不全；今年过年前后，花了几天的时间又整理了一遍，添加了一些书籍相关的信息，就想着要不要也分析下（其实只是统计，并没有分析），算是对这些年自己读书的一个总结。有点遗憾的是，11年的记录只找到10月份到12月份的一部分，10月份以前的则完全没有记录，就没法统计进来了。
但在分析之前，还得说明一下，有些书我没有进行统计，这些书包括以下四种：
国产教材。比如为考研而看的《普通心理学》《心理学导论》之类的，但国外的教材，如《心理学与生活》等，不在此列。
技术类的书。如跟R相关的书，有实体的，也有在线的，都没有被统计进来。
电子书。不论是在手机上，kindle上，还是在更早的汉王上看的电子书，都没有统计进来。在看书这方面，我还是比较传统的，现在基本上只看实体书。
太low的书。如，有套书名叫《卑鄙的圣人：曹操》，老爹看见了，非要买一套，当时只出了5本，就都买了下来。我是家里有的书就要看完的（大概就是看完这套书后改了这个“毛病”），就硬着头皮把这几本书看了一遍。听说这套书让作者赚了一百多万的版税，但这也无法掩盖作者文笔一般、词汇匮乏的事实。印象最深的是，曹操笑起来是“噗嗤”，袁绍笑起来也“噗嗤”，连曹操的老子曹嵩笑起来也“噗嗤”，这到底是一群大老爷们，还是一群小丫头片子啊（当然，用在曹嵩身上也许是合适的）？总之，这类书就不进行统计了。
去掉以上四类书之后，剩下的书（共计465本次），就是要进行分析的了。
首先还是载入分析需要用到的包：
library(tidyverse)library(readxl)library(knitr)然后把数据导入并进行清洗。由于数据已经在excel里整理好了，所以也没啥好清洗的，只是对每本书的字数进行了校正：
book &lt;- read_xlsx(&#39;读书记录.xlsx&#39;) %&gt;% select(year = 2, name = 4, publisher = 6, author = 7, country = 8, dynasty = 9, classification = 10, language = 11, price = 12, page = 13, words = 14, manner = 15) %&gt;% mutate(words = case_when(language %in% c(&#39;古汉&#39;, &#39;英汉&#39;) ~ words*1.3,language %in% c(&#39;古语&#39;, &#39;英语&#39;) ~ words*2,TRUE ~ words),words = ifelse(manner == &#39;书内&#39;, words*.</div>
            </div>
            <div style="clear:both;"></div>
            <hr>
          </div>
        
          <div class="media">
            
            <div class="media-body">
              <a class="link-unstyled" href="/2019/01/pseudo-dynamic-website-scraping/">
                <h3 class="media-heading">（伪）动态网页爬虫-《狗十三》豆瓣短评爬取</h3>
              </a>
              <span class="media-meta">
                <span class="media-date text-small">
                  Jan 1, 2019
                </span>
              </span>
              <div class="media-content hide-xs font-merryweather">这周公司组织了电影赏析，看的电影是《狗十三》。我之前并没有看过这部电影，就想着先去豆瓣上看一下评论。这电影的评论还不少，有好几百条，完全可以全爬下来，分析一下。拉到页面下面，点击后页，url就会跟着变化（start=那里），说明这也不是啥动态网页，完全可以写个循环，用rvest包一页一页的爬。但实际爬取的时候，遇到了问题，就是未登陆的状态下，只能爬前220条评论。我搜索了一下模拟登录的办法，似乎是成功了，但后续该怎么弄，我就不知道了。我在这里卡了一天，没想到解决办法。昨天早上躺在被窝里，突然想到，我之前研究了下用RSelenium爬取动态网页，这里我完全可以先用RSelenium模拟登录，然后把网页当成动态网页爬啊。试了一下，成功了，下面就是相关的操作过程。
首先还是载入需要用的包，要使用RSelenium包，还要先进行一些配置，具体内容可以看RSelenium包的官方网站（这网站好像需要科学上网）：
library(tidyverse)library(RSelenium)library(rvest)library(jiebaR)library(wordcloud2)library(knitr)接下来跟Selenium Server进行连接，这里我用的是Chrome浏览器（变量名rd本应该在第一行，不知道为什么跑到下边去了……）：
 rd &lt;- remoteDriver(remoteServerAddr = &quot;localhost&quot;,port = 4444L,browserName = &quot;chrome&quot;)然后模拟打开豆瓣电影的登录页面，输入用户名和密码，点击登录按键，就可以登录了：
rd$open()rd$navigate(&#39;https://www.douban.com/accounts/login?source=movie&#39;)we &lt;- rd$findElement(using = &#39;xpath&#39;, &#39;//*[@id=&quot;email&quot;]&#39;)we$sendKeysToElement(list(&#39;用户名&#39;))we &lt;- rd$findElement(using = &#39;xpath&#39;, &#39;//*[@id=&quot;password&quot;]&#39;)we$sendKeysToElement(list(&#39;密码&#39;))we &lt;- rd$findElement(using = &#39;xpath&#39;, &#39;//*[@id=&quot;lzform&quot;]/div[6]/input&#39;)we$clickElement()如果没接触过爬虫的，看着上面的代码可能有点懵，但实际上没啥太玄奥的东西。RSelenium包中的函数名就明白显示了它是干什么的，而参数中的那些xpath，在Chrome浏览器中都是可以直接复制出来的。
后面就可以开始爬虫了。我只爬了评价星级、短评时间、有帮助次数和短评文本四项信息。需要说明的是，有些用户虽然写了短评，但不会打分，这种情况下，我认为的将其评价星级定位“无评价”。因为不打分也会影响后面内容的xpath，所以那部分用了一些if条件。另外，虽然不知道会不会用上，在每页的内容爬取完之后，我也会让程序随机休息几秒，省得被轻易地认定为是爬虫程序。
rd$navigate(&#39;https://movie.douban.com/subject/25716096/comments?start=0&amp;limit=20&amp;sort=new_score&amp;status=P&#39;)dog13 &lt;- tibble()for (i in 1:50) {rank &lt;- character(0)time &lt;- character(0)help &lt;- character(0)text &lt;- character(0)temp &lt;- tibble()for (j in 1:20) {xpath_rank &lt;- str_c(&#39;//*[@id=&quot;comments&quot;]/div[&#39;, j, &#39;]/div[2]/h3/span[2]/span[2]&#39;)we &lt;- rd$findElement(using = &#39;xpath&#39;, xpath_rank)rank[j] &lt;- ifelse(str_length(we$getElementAttribute(&#39;title&#39;) %&gt;% unlist()) &gt; 2, &#39;无评价&#39;, we$getElementAttribute(&#39;title&#39;) %&gt;% unlist())if (str_length(we$getElementAttribute(&#39;title&#39;) %&gt;% unlist()) &lt; 3) {xpath_time &lt;- str_c(&#39;//*[@id=&quot;comments&quot;]/div[&#39;, j, &#39;]/div[2]/h3/span[2]/span[3]&#39;)we &lt;- rd$findElement(using = &#39;xpath&#39;, xpath_time)time[j] &lt;- we$getElementText() %&gt;% unlist()} else {xpath_time &lt;- str_c(&#39;//*[@id=&quot;comments&quot;]/div[&#39;, j, &#39;]/div[2]/h3/span[2]/span[2]&#39;)we &lt;- rd$findElement(using = &#39;xpath&#39;, xpath_time)time[j] &lt;- we$getElementText() %&gt;% unlist()}xpath_help &lt;- str_c(&#39;//*[@id=&quot;comments&quot;]/div[&#39;, j, &#39;]/div[2]/h3/span[1]/span&#39;)we &lt;- rd$findElement(using = &#39;xpath&#39;, xpath_help)help[j] &lt;- we$getElementText() %&gt;% unlist()xpath_text &lt;- str_c(&#39;//*[@id=&quot;comments&quot;]/div[&#39;, j, &#39;]/div[2]/p/span&#39;)we &lt;- rd$findElement(using = &#39;xpath&#39;, xpath_text)text[j] &lt;- we$getElementText() %&gt;% unlist()df &lt;- tibble(rank, time, help, text)}dog13 &lt;- bind_rows(dog13, df)rest &lt;- sample(1:10, 1)if (i &lt; 2) {we &lt;- rd$findElement(using = &#39;xpath&#39;, &#39;//*[@id=&quot;paginator&quot;]/a&#39;)we$clickElement()Sys.</div>
            </div>
            <div style="clear:both;"></div>
            <hr>
          </div>
        
          <div class="media">
            
            <div class="media-body">
              <a class="link-unstyled" href="/2018/12/red-envelope/">
                <h3 class="media-heading">使用R语言模拟抢红包</h3>
              </a>
              <span class="media-meta">
                <span class="media-date text-small">
                  Dec 12, 2018
                </span>
              </span>
              <div class="media-content hide-xs font-merryweather">有一次参加了一个特别无聊的讲座，实在是无事可做，就琢磨了一下像微信抢红包那样的机制是如何实现的。自己当时想了一个模拟的方式，出来的结果似乎也可以以假乱真。后来把相关的代码完善了下，用来在自己组织的R语言课上讲for循环和自编函数。现在把这些内容整理出来，权当作一篇小小的教程。
首先假设，有人发了一个200块钱的红包，分给10个人抢：
money &lt;- 200people &lt;- 10给每个人安排一个随机数：
set.seed(181209)rand_number &lt;- sample(1:10000, people, replace = TRUE)rand_number## [1] 4188 591 2386 4520 3692 979 8170 3728 7121 4408随后用每个随机数除以所有随机数的总和得到一个比值，乘以总钱数，进而得到每个人的钱数：
rand_money &lt;- rand_number/sum(rand_number)*moneyrand_money## [1] 21.054219 2.971118 11.995073 22.723274 18.560692 4.921700 41.072820## [8] 18.741674 35.799211 22.160219然后就可以知道具体每个人得到多少钱了：
paste0(paste0(sample(letters, 5, replace = TRUE), collapse = &#39;&#39;),&#39;得到了&#39;, round(rand_money[1], 2), &#39;元，红包剩余&#39;, round(money - sum(rand_money[1:1]), 2), &#39;元。&#39;)## [1] &quot;hdprm得到了21.</div>
            </div>
            <div style="clear:both;"></div>
            <hr>
          </div>
        
          <div class="media">
            
            <div class="media-body">
              <a class="link-unstyled" href="/2018/11/ggplot2-collection/">
                <h3 class="media-heading">ggplot2及其扩展包绘图总结</h3>
              </a>
              <span class="media-meta">
                <span class="media-date text-small">
                  Nov 11, 2018
                </span>
              </span>
              <div class="media-content hide-xs font-merryweather">Bar PlotBox PlotHeatmapHistgramLine ChartMapPie ChartRadar ChartScatter PlotTreemap像这样的教程应该有很多了，但为了自己查阅起来方便，我决定自己也写一个。这里我会尽量多的用到各种theme和palette，省得每次绘图还要一个一个试，看哪个好看（通过这个过程，我可能体验到了女生出门前挑衣服的感觉）。
先把需要用到的包载入：
library(tidyverse)library(ggthemes)Bar Plot直条图应该是最常见的了，在心理学论文中用到直条图时，一般都是把自变量放到x轴上，因变量放到y轴上，然后再添加误差条：
iris %&gt;% group_by(Species) %&gt;% summarise(avg_sl = mean(Sepal.Length), se = sqrt(sd(Sepal.Length)/n())) %&gt;% ggplot(aes(Species, avg_sl, fill = Species)) + geom_col(width = .5) + geom_errorbar(aes(ymin = avg_sl - se, ymax = avg_sl + se),width = .3) + scale_y_continuous(expand = c(0, 0)) + scale_fill_brewer(palette = &#39;Set2&#39;) + labs(y = &#39;Sepal.</div>
            </div>
            <div style="clear:both;"></div>
            <hr>
          </div>
        
          <div class="media">
            
            <div class="media-body">
              <a class="link-unstyled" href="/2018/11/text-analysis-for-tianlong/">
                <h3 class="media-heading">利用文本分析对比两版本天龙八部</h3>
              </a>
              <span class="media-meta">
                <span class="media-date text-small">
                  Nov 11, 2018
                </span>
              </span>
              <div class="media-content hide-xs font-merryweather">今年三月份，为了掌握文本分析技术，特意找了两个版本《天龙八部》的txt文件作为数据而进行练习，但可能被其他事情给耽搁了，当时只完成了一部分。前几天金老去世，令人不胜感概，于是想把这个《天龙八部》的文本分析完成，也算是以自己的方式表达对大师的怀念。
首先还是载入相关的包，这次的包有点多：
library(tidyverse)library(readxl)library(tidytext)library(jiebaR)library(ggthemes)library(widyr)library(igraph)library(ggraph)然后将两个版本的小说文本导入，顺便导入了主要人物的人名，因为这次分析是以分析主要人物为主：
tl_new &lt;- read_lines(&#39;tl_new.txt&#39;)tl_old &lt;- read_lines(&#39;tl_old.txt&#39;)tl_main &lt;- read_lines(&#39;tl_main.txt&#39;) %&gt;% .[-1]因为每个人的称呼不止一个，如乔帮主、萧大王、姊夫等等，都是指萧峰一个人，所以为了统一人名，还要做一些替换工作：
tl_new_tran &lt;- tl_new %&gt;% str_replace_all(&#39;(段公子)|(哥哥)|(誉儿)&#39;, &#39;段誉&#39;) %&gt;%str_replace_all(&#39;(乔峰)|(乔帮主)|(姊夫)|(萧大王)&#39;, &#39;萧峰&#39;) %&gt;% str_replace_all(&#39;(梦郎)|(小和尚)&#39;, &#39;虚竹&#39;) %&gt;% str_replace_all(&#39;(南海鳄神)|(岳老二)&#39;, &#39;岳老三&#39;) %&gt;% str_replace_all(&#39;带头大哥&#39;, &#39;玄慈&#39;) %&gt;% str_replace_all(&#39;延庆太子&#39;, &#39;段延庆&#39;) %&gt;% str_replace_all(&#39;白长老&#39;, &#39;白世镜&#39;) %&gt;% str_replace_all(&#39;全舵主&#39;, &#39;全冠清&#39;) %&gt;% str_replace_all(&#39;甘宝宝&#39;, &#39;钟夫人&#39;) %&gt;% str_replace_all(&#39;小康&#39;, &#39;马夫人&#39;) %&gt;% str_replace_all(&#39;灵儿&#39;, &#39;钟灵&#39;) %&gt;% str_replace_all(&#39;(星宿老怪)|(星宿老仙)&#39;, &#39;丁春秋&#39;) %&gt;% str_replace_all(&#39;庄聚贤&#39;, &#39;游坦之&#39;) %&gt;% str_replace_all(&#39;(慕容公子)|(表哥)&#39;, &#39;慕容复&#39;) %&gt;% str_replace_all(&#39;国师&#39;, &#39;鸠摩智&#39;) %&gt;% str_replace_all(&#39;表妹&#39;, &#39;王语嫣&#39;) %&gt;% str_replace_all(&#39;(婉妹)|(木姊姊)&#39;, &#39;木婉清&#39;) %&gt;% str_replace_all(&#39;(郡主)|(小师妹)&#39;, &#39;阿紫&#39;) %&gt;% str_replace_all(&#39;段王爷&#39;, &#39;段正淳&#39;)tl_old_tran &lt;- tl_old %&gt;% str_replace_all(&#39;(段公子)|(哥哥)|(誉儿)&#39;, &#39;段誉&#39;) %&gt;%str_replace_all(&#39;(乔峰)|(乔帮主)|(姊夫)|(萧大王)&#39;, &#39;萧峰&#39;) %&gt;% str_replace_all(&#39;(梦郎)|(小和尚)&#39;, &#39;虚竹&#39;) %&gt;% str_replace_all(&#39;(南海鳄神)|(岳老二)&#39;, &#39;岳老三&#39;) %&gt;% str_replace_all(&#39;带头大哥&#39;, &#39;玄慈&#39;) %&gt;% str_replace_all(&#39;延庆太子&#39;, &#39;段延庆&#39;) %&gt;% str_replace_all(&#39;白长老&#39;, &#39;白世镜&#39;) %&gt;% str_replace_all(&#39;全舵主&#39;, &#39;全冠清&#39;) %&gt;% str_replace_all(&#39;甘宝宝&#39;, &#39;钟夫人&#39;) %&gt;% str_replace_all(&#39;小康&#39;, &#39;马夫人&#39;) %&gt;% str_replace_all(&#39;灵儿&#39;, &#39;钟灵&#39;) %&gt;% str_replace_all(&#39;(星宿老怪)|(星宿老仙)&#39;, &#39;丁春秋&#39;) %&gt;% str_replace_all(&#39;庄聚贤&#39;, &#39;游坦之&#39;) %&gt;% str_replace_all(&#39;(慕容公子)|(表哥)&#39;, &#39;慕容复&#39;) %&gt;% str_replace_all(&#39;国师&#39;, &#39;鸠摩智&#39;) %&gt;% str_replace_all(&#39;表妹&#39;, &#39;王语嫣&#39;) %&gt;% str_replace_all(&#39;(婉妹)|(木姊姊)&#39;, &#39;木婉清&#39;) %&gt;% str_replace_all(&#39;(郡主)|(小师妹)&#39;, &#39;阿紫&#39;) %&gt;% str_replace_all(&#39;段王爷&#39;, &#39;段正淳&#39;)上面的替换工作并不全，比如，同样是段郞，有时可能是指段誉，有时可能是指段正淳，这就需要具体的情境，才能判断出来这个词指的是谁，但这个工作太麻烦了，这里就放弃了。</div>
            </div>
            <div style="clear:both;"></div>
            <hr>
          </div>
        
          <div class="media">
            
            <div class="media-body">
              <a class="link-unstyled" href="/2018/10/rick-and-morty-heatmap/">
                <h3 class="media-heading">看图写代码：瑞克与莫蒂剧集评分热力图</h3>
              </a>
              <span class="media-meta">
                <span class="media-date text-small">
                  Oct 10, 2018
                </span>
              </span>
              <div class="media-content hide-xs font-merryweather">大概是去年的这个时间，我在一个名叫Data Is Beautiful的reddit论坛上看到了一张Rick and Morty的分集评分热力图，就想用R把它重复出来。当时水平还不怎么样，只能画个大概出来，很多细节都不知道该如何呈现；前几个月，又重新尝试了下，大部分细节都知道该如何实现了，但还是差一点；这里再尝试一下，看看能不能完全重复出来，毕竟这张图应该就是用R画的。
图是这样的：
首先，还是先把需要用到的包载入：
library(tidyverse)然后载入数据：
rm &lt;- read_csv(&quot;rick &amp; morty.csv&quot;) %&gt;% mutate_at(vars(Episode, Season), as.factor)载入数据的时候，为方便后面的绘图，顺便把集数和季数两个变量改成了因子型。具体的数据是这样的：
rm## # A tibble: 31 x 3## Episode Season Rating## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt;## 1 1 1 8.1## 2 2 1 8.7## 3 3 1 8.4## 4 4 1 8.6## 5 5 1 8.9## 6 6 1 9 ## 7 7 1 8.</div>
            </div>
            <div style="clear:both;"></div>
            <hr>
          </div>
        
      </div>
    </div>
    <div class="modal-footer">
      <p class="results-count text-medium"
         data-message-zero="no post found"
         data-message-one="1 post found"
         data-message-other="{n} posts found">
         13 posts found
      </p>
    </div>
  </div>
</div>
    
  
    
    <div id="cover" style="background-image:url('/images/cover.jpg');"></div>
  


    
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js" integrity="sha256-BbhdlvQf/xTY9gja0Dq3HiwQF8LaCRTXxZKRutelT44=" crossorigin="anonymous"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js" integrity="sha256-/BfiIkHlHoVihZdc6TFuj7MmJ0TWcWsMXkeDFwhi0zw=" crossorigin="anonymous"></script>

<script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.7/js/jquery.fancybox.min.js" integrity="sha256-GEAnjcTqVP+vBp3SSc8bEDQqvWAZMiHyUSIorrWwH50=" crossorigin="anonymous"></script>


<script src="/js/script-qi9wbxp2ya2j6p7wx1i6tgavftewndznf4v0hy2gvivk1rxgc3lm7njqb6bz.min.js"></script>



<script>
$(document).ready(function() {
  hljs.configure({ classPrefix: '', useBR: false });
  $('pre.code-highlight > code, pre > code').each(function(i, block) {
    if (!$(this).hasClass('codeblock')) {
      $(this).addClass('codeblock');
    }
    hljs.highlightBlock(block);
  });
});
</script>


  
    
      <script>
        var disqus_config = function () {
          this.page.url = '\/2018\/03\/tackle-english-exam-with-text-mining\/';
          
            this.page.identifier = '\/2018\/03\/tackle-english-exam-with-text-mining\/'
          
        };
        (function() {
          
          
          if (window.location.hostname == "localhost") {
            return;
          }
          var d = document, s = d.createElement('script');
          var disqus_shortname = 'hugo-tranquilpeak-theme';
          s.src = '//' + disqus_shortname + '.disqus.com/embed.js';

          s.setAttribute('data-timestamp', +new Date());
          (d.head || d.body).appendChild(s);
        })();
      </script>
    
  



    
  </body>
</html>

