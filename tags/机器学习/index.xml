<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>机器学习 on RPG</title>
    <link>/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/</link>
    <description>Recent content in 机器学习 on RPG</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 12 May 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>使用机器学习给自己推荐番剧：first try</title>
      <link>/2019/05/anime-recommend-with-ml/</link>
      <pubDate>Sun, 12 May 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/05/anime-recommend-with-ml/</guid>
      <description>A.关于数据B.数据探索b1.番名b2.季数b3.集数b4.年份b5.季节b6.来源b7.类别b8.制作公司b9.评分及人数extra 1.个人、豆瓣与MAL的对比b0.声优extra 2.声优关系网络C.建模与评估c1.清洗数据c2.创建训练数据集和测试数据集c3.朴素贝叶斯算法c4.规则学习算法D.局限与展望花费许多时间学到的东西自然是要用一用的，如果工作中用不到的话，那就用来为生活增添些许乐趣吧。
看番多年，难免遇到烂番，既浪费时间，又影响心情；另外，有些优秀的番剧，可能因为某些原因，与自己失之交臂。要是能在自己看过的番剧的基础上，建立一个模型，帮自己避免烂番，发掘好番，那真是再好不过了。于是我就把自己近年来看过的番剧整理了一下，收集了若干相关信息，做成了excel表格，作为建立模型的原材料。数据是手动整理的，花费的时间比我预计的多很多，但在整理的过程中，也引发了不少回忆，所以也算不上是浪费时间。
A.关于数据数据就是我看番的记录，不全，但应该是足够用了。反应变量是我对某番剧的评分，从5分到10分，是离散数据，在这第一次尝试中，计划将其变为二元分类数据，即5分到7分为不推荐，8分到10分为推荐。预测变量有十来个，包括番剧的年代、类型、制作公司、声优和网站评分等信息，在第一次尝试中，计划把数据弄成稀疏矩阵，使用朴素贝叶斯算法和规则学习算法来进行分类。
在查看数据之前，先载入分析需要用到的包：
library(tidyverse)library(readxl)library(here)library(ggthemes)library(corrplot)library(tidytext)library(widyr)library(igraph)library(ggraph)library(e1071)library(RWeka)library(gmodels)然后导入数据，并进行初步的清洗：
anime &amp;lt;- read_xlsx(here(&amp;#39;content&amp;#39;, &amp;#39;post&amp;#39;, &amp;#39;data&amp;#39;, &amp;#39;anime_record.xlsx&amp;#39;)) %&amp;gt;%mutate_all(str_remove_all, pattern = &amp;#39;\U00A0&amp;#39;) %&amp;gt;% # 去掉不间断空格&amp;lt;U+00A0&amp;gt;select(-record_time) %&amp;gt;%mutate(studio = str_remove(studio, &amp;#39;,.*&amp;#39;)) %&amp;gt;%mutate_at(vars(c(&amp;#39;season_number&amp;#39;, &amp;#39;episode&amp;#39;, &amp;#39;year&amp;#39;, &amp;#39;rating&amp;#39;, &amp;#39;db_number&amp;#39;, &amp;#39;mal_number&amp;#39;)), as.</description>
    </item>
    
    <item>
      <title>《机器学习与R语言》学习笔记02：朴素贝叶斯</title>
      <link>/2019/04/mlr-note-nb/</link>
      <pubDate>Thu, 25 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/04/mlr-note-nb/</guid>
      <description>书中的第二个例子是利用朴素贝叶斯算法判断垃圾短信。
首先载入需要用到的包：
library(tidyverse) # 清洗数据library(here) # 设置数据文件路径library(tidytext) # 分词及创建稀疏矩阵library(e1071) # 建模library(gmodels) # 评估模型在清洗数据的时候遇到一定的困难，因为书中是用tm包进行文本处理的，而我完全没有用过这个包（甚至也没有装这个包），所以看书中的代码就只能凭感觉脑补了。不过，还好，最后还是成功写出了tidyverse化的数据清洗代码，如下：
sms &amp;lt;- read_csv(here(&amp;#39;content&amp;#39;, &amp;#39;post&amp;#39;, &amp;#39;data&amp;#39;, &amp;#39;02-sms_spam.csv&amp;#39;)) %&amp;gt;% mutate(type = factor(type),row = row_number()) %&amp;gt;% unnest_tokens(word, text) %&amp;gt;% anti_join(stop_words) %&amp;gt;% filter(!str_detect(word, &amp;#39;\\d&amp;#39;)) %&amp;gt;% cast_sparse(row, word) %&amp;gt;% as.matrix() %&amp;gt;% as_tibble() %&amp;gt;% select(which(colSums(.) &amp;gt; 4)) %&amp;gt;% bind_cols(read_csv(here(&amp;#39;data&amp;#39;, &amp;#39;02-sms_spam.csv&amp;#39;)) %&amp;gt;% mutate(type = factor(type),row = row_number()) %&amp;gt;% unnest_tokens(word, text) %&amp;gt;% anti_join(stop_words) %&amp;gt;% filter(!str_detect(word, &amp;#39;\\d&amp;#39;)) %&amp;gt;%select(-3) %&amp;gt;% distinct()) %&amp;gt;% mutate_if(is.</description>
    </item>
    
    <item>
      <title>《机器学习与R语言》学习笔记01：kNN</title>
      <link>/2019/04/mlr-note-knn/</link>
      <pubDate>Tue, 23 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/04/mlr-note-knn/</guid>
      <description>通过将《机器学习与R语言》一书中的代码tidyverse化，来学习这本书。
书中第一个例子是利用kNN算法来诊断乳腺癌。
首先载入需要用到的包：
library(tidyverse) # 清洗数据library(here) # 设置数据文件路径library(knitr) # 呈现更好看的表格library(kableExtra) # 同上library(class) # 使用包中的knn()函数library(gmodels) # 使用包中的CrossTable()函数然后导入数据并清洗：
wbcd &amp;lt;- read_csv(here(&amp;#39;content&amp;#39;, &amp;#39;post&amp;#39;, &amp;#39;data&amp;#39;, &amp;#39;01-wisc_bc_data.csv&amp;#39;)) %&amp;gt;% select(-id) %&amp;gt;% mutate(diagnosis = factor(diagnosis, levels = c(&amp;#39;B&amp;#39;, &amp;#39;M&amp;#39;),labels = c(&amp;#39;Benign&amp;#39;, &amp;#39;Malignant&amp;#39;))) %&amp;gt;% mutate_if(is.numeric, ~ (.x - min(.x)) / (max(.x) - min(.x)))首先使用here函数找到数据文件的路径，然后使用read_csv函数将其读入R中；随后通过select函数将id变量去掉；然后利用mutate函数将diagnosis变量改为因子型；最后利用mutate_if函数，将所有数值型的变量进行min-max标准化，这里用到了公式化的匿名函数，可以使代码更为简练。此时的数据是这样的：
wbcd %&amp;gt;% head() %&amp;gt;% kable() %&amp;gt;% kable_styling(bootstrap_options = &amp;quot;striped&amp;quot;, font_size = 12) %&amp;gt;%scroll_box(width = &amp;quot;100%&amp;quot;) diagnosisradius_meantexture_meanperimeter_meanarea_meansmoothness_meancompactness_meanconcavity_meanconcave points_meansymmetry_meanfractal_dimension_meanradius_setexture_seperimeter_searea_sesmoothness_secompactness_seconcavity_seconcave points_sesymmetry_sefractal_dimension_seradius_worsttexture_worstperimeter_worstarea_worstsmoothness_worstcompactness_worstconcavity_worstconcave points_worstsymmetry_worstfractal_dimension_worstMalignant0.</description>
    </item>
    
  </channel>
</rss>